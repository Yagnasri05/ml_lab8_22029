{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la4_8JzFaedV",
        "outputId": "25afa3dc-644e-4483-9bc8-2b5e725dc697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu5VjF4-aHmj"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-io matplotlib xgboost catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary libraries\n",
        "import os\n",
        "import itertools\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import numpy as np\n",
        "import pywt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "#loading audio file function\n",
        "def load_wav_16k_mono(filename):\n",
        "    # Load encoded wav file\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    # Decode wav (tensors by channels)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
        "    # Removes trailing axis\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    # Goes from 44100Hz to 16000hz - amplitude of the audio signal\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav\n",
        "\n",
        "#listing gunshot n non gunshot files\n",
        "POS = '/content/drive/MyDrive/aidataset/gunshot'\n",
        "NEG = '/content/drive/MyDrive/aidataset/nongunshot'\n",
        "pos = tf.data.Dataset.list_files(POS+'/*.wav')\n",
        "neg = tf.data.Dataset.list_files(NEG+'/*.wav')\n",
        "\n",
        "#labeling gunshot as 1 and non gunshot as 0\n",
        "positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))\n",
        "negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))\n",
        "#calculating the quotient n remainder\n",
        "repeat_count = len(negatives) // len(positives)\n",
        "remainder = len(negatives) % len(positives)\n",
        "#oversampling the minority with given values of quotient n remainder\n",
        "new_positives = positives.repeat(repeat_count) #repeat that many times as quotient\n",
        "new_positives = new_positives.concatenate(positives.take(remainder)) #add that many as remainder\n",
        "data = new_positives.concatenate(negatives) #concatenate gunshot n nonshot\n",
        "\n",
        "# function provided in tenserflow\n",
        "def preprocess(file_path, label):\n",
        "    wav = load_wav_16k_mono(file_path)\n",
        "    wav = wav[:40000]\n",
        "    #padding to make equal wav\n",
        "    zero_padding = tf.zeros([40000] - tf.shape(wav), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, wav], 0)\n",
        "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32) #short term fourier transform\n",
        "    spectrogram = tf.abs(spectrogram) #taking absolute of it\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram, label\n",
        "\n",
        "#preprocess, shuffling n batch for data\n",
        "data = data.map(preprocess)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=5000)\n",
        "data = data.batch(16)\n",
        "data = data.prefetch(8)\n",
        "\n",
        "#splitting of data into train, validation and test\n",
        "train = data.take(130)\n",
        "test = data.skip(130).take(30)\n",
        "ttest = data.skip(160).take(20)\n",
        "\n",
        "\n",
        "#Custom CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(1241, 257, 1)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "model.add(Dropout(0.5))  # Add dropout layer with dropout rate of 0.5\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Add dropout layer with dropout rate of 0.5\n",
        "\n",
        "# Create a feature extraction model\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.layers[-1].output)\n",
        "\n",
        "# Extract features from train in the dataset\n",
        "features = []\n",
        "labels = []\n",
        "for spectrogram, label in train:\n",
        "    extracted_features = feature_extractor.predict(spectrogram)\n",
        "    features.append(extracted_features)\n",
        "    labels.append(label)\n",
        "\n",
        "features = np.concatenate(features, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "# Extract features from test set\n",
        "test_features = []\n",
        "test_labels = []\n",
        "for spectrogram, label in test:\n",
        "    extracted_features = feature_extractor.predict(spectrogram)\n",
        "    test_features.append(extracted_features)\n",
        "    test_labels.append(label)\n",
        "\n",
        "test_features = np.concatenate(test_features, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)"
      ],
      "metadata": {
        "id": "_9M91yusaZVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the entropy value\n",
        "def calc_entropy(labels):\n",
        "    # H= - sum(p * log(p))\n",
        "    unique_l, label_counts = np.unique(labels, return_counts=True) #getting count of 1 and 0\n",
        "    prob = label_counts / len(labels) #Probabily of 0 and 1\n",
        "    entropy = -np.sum(prob * np.log2(prob))\n",
        "    return entropy\n",
        "\n",
        "# Calculate Information Gain for a specific feature\n",
        "def calc_info_gain(data, labels, feature_index):\n",
        "    # IG= H(before ) - H(after)\n",
        "    before_H = calc_entropy(labels) # calculating before entropy ie before spliting\n",
        "    unique_values, value_counts = np.unique(data[:, feature_index], return_counts=True) #get unique feature value and its count\n",
        "\n",
        "    after_H = 0\n",
        "    for value, count in zip(unique_values, value_counts):\n",
        "      #split data based on feature value\n",
        "        child_labels = labels[data[:, feature_index] == value] # Get labels for the split data with this feature value\n",
        "        after_H += (count / len(labels)) * calc_entropy(child_labels) #calculate weighted sum of entropy after split\n",
        "\n",
        "    info_gain = before_H - after_H #information gain\n",
        "    return info_gain\n",
        "\n",
        "#Finding root node with feature with info gain highest\n",
        "def find_root_node(data, labels):\n",
        "    num_features = data.shape[1] #no of features\n",
        "    #intialize info gain and feature index\n",
        "    best_info_gain = -1\n",
        "    best_feature_idx = -1\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        info_gain = calc_info_gain(data, labels, feature_index) #calculate info gain for current feature\n",
        "        if info_gain > best_info_gain:  #if cur_ig is more than best then cur_ig is best one also feature index\n",
        "            best_info_gain = info_gain\n",
        "            best_feature_idx = feature_index\n",
        "\n",
        "    return best_feature_index\n",
        "\n",
        "root_node_index = find_root_node(features, labels)\n",
        "print(\"Root node feature index:\", root_node_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMoc1KTsc45I",
        "outputId": "360afec5-82f4-414b-cbcf-0d137851d0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root node feature index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A2. Function to bin continuous features into categorical features\n",
        "def bin_cont_feature(feature_col, num_bins=10, bin_type='equal_width'):\n",
        "    if bin_type == 'equal_width': # Divide range into equal width bins\n",
        "        bin_edges = np.linspace(np.min(feature_col), np.max(feature_col), num_bins + 1)\n",
        "    elif bin_type == 'frequency': # Divide based on frequency of values\n",
        "        bin_edges = np.histogram_bin_edges(feature_col, bins=num_bins, range=(np.min(feature_col), np.max(feature_col)))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid binning type. Choose 'equal_width' or 'frequency'.\")\n",
        "\n",
        "    binned_features = np.digitize(feature_col, bins=bin_edges) - 1 # Assign each value to its corresponding bin\n",
        "    return binned_features, bin_edges\n",
        "\n",
        "\n",
        "# A3. Custom Decision Tree module\n",
        "class DecisionTree:\n",
        "    def __init__(self):\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "       # Build the decision tree\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "      # predict labels for provided data using built tree\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            predictions.append(self._traverse_tree(sample, self.tree))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _build_tree(self, X, y):\n",
        "        # Base case: if all labels are the same, return leaf node\n",
        "        if len(set(y)) == 1:\n",
        "            return {'class': y[0]}\n",
        "\n",
        "        # Find the best feature to split on\n",
        "        best_feature_index = find_root_node(X, y) #root is feature with highest info gain\n",
        "        best_feature_values = X[:, best_feature_index] #get its values\n",
        "        unique_values = np.unique(best_feature_values)\n",
        "\n",
        "        # Initialize the tree\n",
        "        tree = {'feature_index': best_feature_index, 'children': {}}\n",
        "\n",
        "        # Recursively build subtrees\n",
        "        for value in unique_values:\n",
        "            subset_idx = np.where(best_feature_values == value)[0] #get indices of samples with particular feature values\n",
        "            subset_X = X[subset_idx]\n",
        "            subset_y = y[subset_idx]\n",
        "            tree['children'][value] = self._build_tree(subset_X, subset_y) # Build subtree for the current feature value\n",
        "\n",
        "        return tree\n",
        "\n",
        "    def _traverse_tree(self, sample, node):\n",
        "      # Traverse the decision tree to predict the label for a given sample\n",
        "        if 'class' in node: #if the node is a leaf node return the class label\n",
        "            return node['class']\n",
        "        else:\n",
        "            feature_value = sample[node['feature_index']] # Get the value of the feature at the current node\n",
        "            if feature_value in node['children']:\n",
        "                return self._traverse_tree(sample, node['children'][feature_value]) # Recursively traverse subtree\n",
        "            else:\n",
        "                # If the feature value is not in training data, return majority class\n",
        "                return max(node['children'], key=lambda x: len(node['children'][x]))\n",
        "\n",
        "# Instantiate and train the custom decision tree\n",
        "custom_tree = DecisionTree()\n",
        "custom_tree.fit(features, labels)\n",
        "predictions = custom_tree.predict(test_features)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "precision = precision_score(test_labels, predictions)\n",
        "recall = recall_score(test_labels, predictions)\n",
        "conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Determine the best feature index to bin based on information gain\n",
        "best_binning_feature_index = find_root_node(features, labels)\n",
        "\n",
        "# Bin the best feature\n",
        "feature_needing_binning = features[:, best_binning_feature_index]\n",
        "binned_feature, bin_edges = bin_cont_feature(feature_needing_binning)\n",
        "# Replace the original feature column with the binned feature\n",
        "features[:, best_binning_feature_index] = binned_feature\n",
        "\n",
        "# Train the custom decision tree with binned features\n",
        "custom_tree = DecisionTree()\n",
        "custom_tree.fit(features, labels)\n",
        "predictions = custom_tree.predict(test_features)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "precision = precision_score(test_labels, predictions)\n",
        "recall = recall_score(test_labels, predictions)\n",
        "conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRf4d7VHnP_G",
        "outputId": "af8c3a49-fa04-4575-9f7d-04ca0e84d8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9395833333333333\n",
            "F1 Score: 0.9376344086021505\n",
            "Precision: 1.0\n",
            "Recall: 0.8825910931174089\n",
            "Confusion Matrix:\n",
            "[[233   0]\n",
            " [ 29 218]]\n",
            "Accuracy: 0.9375\n",
            "F1 Score: 0.9356223175965664\n",
            "Precision: 0.9954337899543378\n",
            "Recall: 0.8825910931174089\n",
            "Confusion Matrix:\n",
            "[[232   1]\n",
            " [ 29 218]]\n"
          ]
        }
      ]
    }
  ]
}